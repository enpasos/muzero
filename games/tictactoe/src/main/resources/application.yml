logging:
  file:
    name: logs/muzero-tictactoe.log
    size: 100MB
  pattern:
    console: "%d %-5level [%thread] %logger{0} [%line]: %msg%n"
    file: "%d %-5level [%thread] %logger [%line]: %msg%n"
  level:
    # org.hibernate.SQL: DEBUG
    # org.hibernate.type: TRACE
    root: WARN
    ai.enpasos: DEBUG
    ai.enpasos.muzero.platform.agent.d_model.service.ModelController: INFO
    ai.djl: INFO
spring:
  datasource:
    username: muzero
    password: hd7sge
    url: jdbc:postgresql://localhost:5432/muzero
  jpa:
    properties.hibernate:
      #jdbc.lob.non_contextual_creation: true
      #dialect: org.hibernate.dialect.PostgreSQLDialect
    hibernate: # Hibernate ddl auto (create, create-drop, validate, update)
      ddl-auto: update
      jdbc:
        batch_size: 2000
      order_updates: true
      order_inserts: true

muzero:
  activeGame: TICTACTOE
  run: train
  games:
    TICTACTOE:
      modelName: MuZero-TicTacToe
      gameClassName: ai.enpasos.muzero.tictactoe.config.TicTacToeGame
      actionClassName: ai.enpasos.muzero.tictactoe.config.TicTacToeAction
      playerMode: TWO_PLAYERS
      valueInterval: [ -1,1 ]
      numObservationLayers: 5
      numActionLayers: 1


      numberOfTrainingStepsPerEpoch: 40

      numberOfTrainingSamplesPerRuleTrainingEpoch: 10000
      numberTrainingBoxes: 5

      numUnrollSteps: 5
      maxUnrollSteps: 9

      discount: 1.0
      weightDecay: 0.0001
      lrInit: 0.0001

      # numberTrainingStepsOnStart: 0

      knownBoundsType: FROM_VALUES

      inferenceDeviceType: GPU
      outputDir: ./memory/tictactoe/
      size: 3
      maxMoves: 9  # size*size
      boardHeight: 3 # size
      boardWidth: 3 # size
      actionSpaceSize: 9  # size*size

      # without symmetry usage
      # symmetryType: NONE
      # batchSize: 2048

      # using the square symmetry of the board
      symmetryType: SQUARE
      batchSize: 512

      # Gumbel MuZero parameters
      initialGumbelM: 4
      cVisit: 20  # 50 in paper
      cScale: 1.0  # 1.0 in paper

      numResiduals: 1
      broadcastEveryN: 14  # out of range here


      numChannelsRules: 30
      numChannelsPolicy: 30
      numChannelsValue: 30

      numCompressedChannelsRules: 3
      numCompressedChannelsPolicy: 3
      numCompressedChannelsValue: 3


      valueLossWeight: 1

      withConsistencyLoss: false  # check if not to remove completely
      consistencyLossWeight: 2
      # legalActionsLossWeight: 0.1111  # 1/actionSpaceSize

      numberOfTrainingSteps: 1000000

      windowSize: 10000
      initialRandomEpisodes: 100000

      numChannelsHiddenLayerSimilarityProjector: 50
      numChannelsOutputLayerSimilarityProjector: 50
      numChannelsHiddenLayerSimilarityPredictor: 25
      numChannelsOutputLayerSimilarityPredictor: 50


      offPolicyCorrectionOn: true
      allOrNothingOn: true
      offPolicyRatioLimit: 1.0

      numParallelInferences: 1000


      rewardLossThreshold: 0.01
      legalActionLossMaxThreshold: 0.3


      playTypes:
        PLAYOUT:
          forTraining: false
          numSimulations: 20
          rootExplorationFraction: 0.0  # 0.0 means switched off
          temperatureRoot: 0.0
          gumbelActionSelection: true
          withGumbel: false
        HYBRID:
          forTraining: true
          numParallelGamesPlayed: 1000
          tdSteps: 9 # 9 # size * size
          forTdStep0ValueTraining: false
          numSimulations: 20
          numSimulationsHybrid: 20

          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
          # dirichlet noise is added before mcts
          rootDirichletAlpha: 1.2
          rootExplorationFraction: 0.25  # 0.0 means switched off

          # temperatureRoot applies a temperature to the policy target after mcts and after storing it in game statistics but
          # before drawing the action from the policy target (different from the paper)
          # it helps to explore the game tree where the policy rules out bad moves and makes the agent blind to what could happen after such a move
          temperatureRoot: 10.0
          gumbelActionSelection: true

          fractionOfPureExplorationAdded: 0.3
          fractionOfPureExploitationAdded: 0.0
#        REANALYSE:
#          forTraining: true
#          numParallelGamesPlayed: 1000
#          tdSteps: 9
#          numSimulations: 20
#          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
#          # dirichlet noise is added before mcts
#          rootDirichletAlpha: 1.2
#          rootExplorationFraction: 0.25  # 0.0 means switched off
#        BEST_EFFORT:
#          forTraining: true
#          numParallelGamesPlayed: 1000
#          tdSteps: 9 # 9 # size * size
#          numSimulations: 20
#          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
#          # dirichlet noise is added before mcts
#          rootDirichletAlpha: 1.2
#          rootExplorationFraction: 0.25  # 0.0 means switched off
#          # find difference to HYBRID
#          temperatureRoot: 1.0
#          gumbelActionSelection: false
#          # gumbelActionSelection: true
