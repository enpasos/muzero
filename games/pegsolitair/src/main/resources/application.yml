logging:
  level.root: WARN
  level.ai.enpasos: DEBUG

spring:
  datasource:
    username: muzero
    password: hd7sge
    url: jdbc:postgresql://localhost:5432/muzero
  jpa:
    properties.hibernate:
      #jdbc.lob.non_contextual_creation: true
      dialect: org.hibernate.dialect.PostgreSQLDialect
    hibernate: # Hibernate ddl auto (create, create-drop, validate, update)
      ddl-auto: update
      jdbc:
        batch_size: 2000
      order_updates: true
      order_inserts: true

muzero:
  activeGame: PEG_SOLITAIR
  run: train
  games:
    PEG_SOLITAIR:
      outputDir: ./memory/pegsolitair/
      values: [ -25,25 ]

      numSimulations: 100
      numParallelGamesPlayed: 1000

      # general
      modelName: MuZero-PegSolitair
      gameClassName: ai.enpasos.muzero.pegsolitair.config.PegSolitairGame
      actionClassName: ai.enpasos.muzero.pegsolitair.config.PegSolitairAction
      playerMode: SINGLE_PLAYER
      networkWithRewardHead: false
      inferenceDeviceType: GPU

      # game/environment
      size: 7
      maxMoves: 49  # size*size
      boardHeight: 7 # size
      boardWidth: 7 # size
      actionSpaceSize: 196  # size * size * 4 = point to start from and 4 directions

      # network sizing
      numObservationLayers: 1
      numActionLayers: 4   # one for each direction


      bottleneckCompression: 0.75  # out of range here

      representation:
        numResiduals: 2
        rules:
          numChannels: 100
          numChannelsState: 3
        policy:
          numChannels: 1        # later 0
          numChannelsState: 1   # later 0
        value:
          numChannels: 1        # later 0
          numChannelsState: 1   # later 0

      generation:
        numResiduals: 2
        rules:
          numChannels: 100
          numChannelsState: 3
        policy:
          numChannels: 1        # later 0
          numChannelsState: 1   # later 0
        value:
          numChannels: 1        # later 0
          numChannelsState: 1   # later 0

      prediction:
        numResiduals: 8
        rules:
          numChannels: 100
          numChannelsState: 100
        policy:
          numChannels: 100
          numChannelsState: 100
        value:
          numChannels: 100
          numChannelsState: 100


      # network training
      # symmetryType: NONE
      # batchSize: 512
      symmetryType: SQUARE
      batchSize: 64

      numberOfTrainingStepsPerEpoch: 100

      numberOfTrainingSteps: 100000

      windowSize: 10000
      numUnrollSteps: 5
      tdSteps: 196 # here equals actionSpaceSize
      discount: 1.0
      # loss details
      weightDecay: 0.0001
      valueLossWeight: 1.0
      # network training - adam optimizer
      lrInit: 0.0001

      # play
      numberTrainingStepsOnStart: 0

      knownBoundsType: NONE





