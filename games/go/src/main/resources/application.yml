logging:
  file:
    name: logs/muzero-go.log
  pattern:
    console: "%d %-5level %logger{0} : %msg%n"
    file: "%d %-5level [%thread] %logger : %msg%n"
  level:
    root: WARN
    ai.enpasos: INFO

muzero:
  activeGame: GO_5
  run: train
  games:
    GO_5:
      modelName: MuZero-Go-5
      outputDir: ./memory/go5/
      values: [-25,-24,-23,-22,-21,-20,-19,-18,-17,-16,-15,-14,-13,-12,-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]
      komi: 0.5
      maxKomi: 6.5
      size: 5
      boardHeight: 5 # size
      boardWidth: 5 # size
      actionSpaceSize: 26  # size * size + 1,  place a stone on the board or pass
      numParallelGamesPlayed: 250
      numEpisodes: 4

      # general
      gameClassName: ai.enpasos.muzero.go.config.GoGame
      actionClassName: ai.enpasos.muzero.go.config.GoAction
      playerMode: TWO_PLAYERS
      valueHeadType: DISTRIBUTION  # EXPECTED
      networkWithRewardHead: false
      withRewardHead: false
      absorbingStateDropToZero: false
      inferenceDeviceType: GPU

      # game/environment
      maxMoves: 10000


      # network sizing
      numObservationLayers: 17  # 8 history * 2 player + 1 color of next player
      numActionLayers: 1
      numChannels: 128     # 256 in the paper
      numHiddenStateChannels: 19 # squeezing the hidden state from c to observationLayers + 2
      numResiduals: 16
      squeezeChannelRatio: 10  # on squeeze and excitation (SE) block

      # network training
      symmetryType: SQUARE
      numberOfTrainingStepsPerEpoch: 100
      # windowValueSelfconsistencySize: 30000   <- postponed
      batchSize: 128
      numUnrollSteps: 5
      tdSteps: 10000 # here equals max moves
      discount: 1.0
      # loss details
      weightDecay: 0.0001
      valueLossWeight: 1.0
      # network training - adam optimizer
      lrInit: 0.0001

      # play
      numberTrainingStepsOnStart: 1000

      knownBoundsType: BOARD_GAME


      gameBufferWritingFormat: ZIPPED_PROTOCOL_BUFFERS # alternative: ZIPPED_JSON


      # Gumbel MuZero parameters
      initialGumbelM: 8
      cVisit: 50  # 50 in paper
      cScale: 1.0  # 1.0 in paper

      numSimulations: 50
      numberOfTrainingSteps: 300000


      broadcastEveryN: 8
      numBottleneckChannels: 64
      windowSize: 40000
      maxGameLiveTime: 40000

      surpriseHandlingOn: false # false in paper
      extraValueTrainingOn: true # false in paper

      gumbelSoftmaxTemperature: 1.0 # 0.0 -> max/gumble trick to draw as in paper; infinite -> random distributed




    GO_9:
      modelName: MuZero-Go-9
      values: [-25,-24,-23,-22,-21,-20,-19,-18,-17,-16,-15,-14,-13,-12,-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]

      outputDir: ./memory/go9/

      komi: 0.5
      maxKomi: 6.5
      size: 9
      boardHeight: 9 # size
      boardWidth: 9 # size
      actionSpaceSize: 82  # size * size + 1,  place a stone on the board or pass
      numChannels: 128
      batchSize: 64
      numParallelGamesPlayed: 100
      numEpisodes: 10
      squeezeChannelRatio: 10  # on squeeze and excitation (SE) block

      # general
      gameClassName: ai.enpasos.muzero.go.config.GoGame
      actionClassName: ai.enpasos.muzero.go.config.GoAction
      playerMode: TWO_PLAYERS
      valueHeadType: DISTRIBUTION  # EXPECTED
      networkWithRewardHead: false
      withRewardHead: false
      absorbingStateDropToZero: false # faster for integration test
      inferenceDeviceType: GPU

      # game/environment
      maxMoves: 10000


      # network sizing
      numObservationLayers: 17  # 8 history * 2 player + 1 color of next player
      numActionLayers: 1

      numHiddenStateChannels: 19 # squeezing the hidden state from c to observationLayers + 2
      numResiduals: 16

      # network training
      symmetryType: SQUARE
      numberOfTrainingStepsPerEpoch: 100
      numUnrollSteps: 5
      tdSteps: 10000 # here equals max moves
      discount: 1.0
      # loss details
      weightDecay: 0.0001
      valueLossWeight: 1.0
      # network training - adam optimizer
      lrInit: 0.0001

      # play
      numberTrainingStepsOnRandomPlay: 0

      knownBoundsType: BOARD_GAME


      gameBufferWritingFormat: ZIPPED_PROTOCOL_BUFFERS # alternative: ZIPPED_JSON


      # Gumbel MuZero parameters
      initialGumbelM: 16
      cVisit: 50  # 50 in paper
      cScale: 1.0  # 1.0 in paper

      numSimulations: 50
      numberOfTrainingSteps: 300000


      broadcastEveryN: 8
      numBottleneckChannels: 64
      windowSize: 40000
      maxGameLiveTime: 40000

      surpriseHandlingOn: false # false in paper
      extraValueTrainingOn: false # false in paper

      gumbelSoftmaxTemperature: 1.0 # 0.0 -> max/gumble trick to draw as in paper; infinite -> random distributed

