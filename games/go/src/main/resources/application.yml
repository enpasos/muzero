logging:
  file.name: logs/muzero-go.log
  pattern:
    console: "%d %-5level %logger{0} : %msg%n"
    file: "%d %-5level [%thread] %logger : %msg%n"
  level:
    root: WARN
    ai.enpasos: INFO

muzero:
  activeGame: GO_9
  run: train
  games:
    GO_5:
      modelName: MuZero-Go-5
      outputDir: ./memory/go5/
      valueInterval: [ -25,25 ]
      komi: 0.5
      maxKomi: 6.5
      size: 5
      boardHeight: 5 # size
      boardWidth: 5 # size
      actionSpaceSize: 26  # size * size + 1,  place a stone on the board or pass


      # general
      gameClassName: ai.enpasos.muzero.go.config.GoGame
      actionClassName: ai.enpasos.muzero.go.config.GoAction
      playerMode: TWO_PLAYERS
      networkWithRewardHead: false
      withRewardHead: false
      absorbingStateDropToZero: false
      inferenceDeviceType: GPU

      # game/environment
      maxMoves: 10000


      # network sizing
      numObservationLayers: 17  # 8 history * 2 player + 1 color of next player
      numActionLayers: 1


      # network training
      symmetryType: SQUARE
      numberOfTrainingStepsPerEpoch: 100
      # windowValueSelfconsistencySize: 30000   <- postponed
      batchSize: 64
      numUnrollSteps: 5
      # tdSteps: 10000 # here equals max moves
      discount: 1.0
      # loss details
      weightDecay: 0.0001
      # network training - adam optimizer
      lrInit: 0.0001

      # play
      numberTrainingStepsOnStart: 0 # 2000

      knownBoundsType: FROM_VALUES


      gameBufferWritingFormat: ZIPPED_PROTOCOL_BUFFERS # alternative: ZIPPED_JSON


      # Gumbel MuZero parameters
      initialGumbelM: 8
      cVisit: 50  # 50 in paper
      cScale: 1.0  # 1.0 in paper

      numChannels: 256     # 256 in the paper
      numBottleneckChannels: 128
#      numResiduals: 12
#      broadcastEveryN: 8  # broadcasting block in every 8th layer


      numResidualsRepresentation: 12
      broadcastEveryNRepresentation: 8  # out of range here

      numResidualsGeneration: 12
      broadcastEveryNGeneration: 8 # out of range here

      surpriseHandlingOn: false # false in paper
      extraValueTrainingOn: false # false in paper

      valueLossWeight: 1.0

      maxGameLiveTime: 10000000  # or 40000
      surpriseCheckInterval: 50000

      windowSizeStart: 40000
      windowSizeIsDynamic: false

      numberOfTrainingSteps: 1000000
    #  numberOfEpisodesPerJVMStart: 40 # 50 would be ok


      numSimMin: 5
      numSimWindow: 5
      numSimMax: 800
      numSimThreshold: 0.001

      numChannelsHiddenLayerSimilarity: 512
      numChannelsOutputLayerSimilarity: 1024

      playTypes:
        PLAYOUT:
          forTraining: false
          numSimulations: 50
          rootExplorationFraction: 0.0  # 0.0 means switched off
          temperatureRoot: 0.0
          gumbelActionSelection: false
        HYBRID:
          forTraining: true
          numEpisodes: 1
          numParallelGamesPlayed: 500
          tdSteps:  10000
          forTdStep0ValueTraining: false
          forTdStep0PolicyTraining: true
          numSimulations: 30
          numSimulationsHybrid: 30

          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
          # dirichlet noise is added before mcts
          rootDirichletAlpha: 0.4
          rootExplorationFraction: 0.25  # 0.0 means switched off

          # temperatureRoot applies a temperature to the policy target after mcts and after storing it in game statistics but
          # before drawing the action from the policy target (a little different from the paper)
          # it helps to explore the game tree where the policy rules out bad moves and makes the agent blind to what could happen after such a move
          temperatureRoot: 2.0
          gumbelActionSelection: false

        BEST_EFFORT:
          forTraining: true
          numEpisodes: 1
          numParallelGamesPlayed: 500
          tdSteps: 10000
          numSimulations: 30
          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
          # dirichlet noise is added before mcts
          rootDirichletAlpha: 0.4
          rootExplorationFraction: 0.25  # 0.0 means switched off
          gumbelActionSelection: true


    GO_9:
      modelName: MuZero-Go-9
      valueInterval: [ -81,81 ]
      outputDir: ./memory/go9/

      komi: 0.5
      maxKomi: 6.5
      size: 9
      boardHeight: 9 # size
      boardWidth: 9 # size
      actionSpaceSize: 82  # size * size + 1,  place a stone on the board or pass
      batchSize: 32


      # general
      gameClassName: ai.enpasos.muzero.go.config.GoGame
      actionClassName: ai.enpasos.muzero.go.config.GoAction
      playerMode: TWO_PLAYERS
      networkWithRewardHead: false
      withRewardHead: false
      absorbingStateDropToZero: false
      inferenceDeviceType: GPU

      # game/environment
      maxMoves: 10000


      # network sizing
      numObservationLayers: 17  # 8 history * 2 player + 1 color of next player
      numActionLayers: 1



      # network training
      symmetryType: SQUARE
      numberOfTrainingStepsPerEpoch: 100

      numUnrollSteps: 5
      # tdSteps: 10000 # here equals max moves
      discount: 1.0
      # loss details
      weightDecay: 0.0001
      # network training - adam optimizer
      lrInit: 0.0001

      # play
      numberTrainingStepsOnStart: 0 # 2000

      knownBoundsType: FROM_VALUES


      gameBufferWritingFormat: ZIPPED_PROTOCOL_BUFFERS # alternative: ZIPPED_JSON


      # Gumbel MuZero parameters
      initialGumbelM: 16
      cVisit: 50  # 50 in paper
      cScale: 1.0  # 1.0 in paper

      numChannels: 128     # 256 in the paper
      numBottleneckChannels: 64


      numResidualsRepresentation: 12
      broadcastEveryNRepresentation: 8  # out of range here

      numResidualsGeneration: 12
      broadcastEveryNGeneration: 8 # out of range here

      surpriseHandlingOn: false # false in paper
      extraValueTrainingOn: false # false in paper

      valueLossWeight: 1.0

      maxGameLiveTime: 10000000  # or 40000
      surpriseCheckInterval: 50000

      windowSizeStart: 40000
      windowSizeIsDynamic: false

      numberOfTrainingSteps: 1000000
      #  numberOfEpisodesPerJVMStart: 40 # 50 would be ok


      numSimMin: 5
      numSimWindow: 5
      numSimMax: 800
      numSimThreshold: 0.001

      numChannelsHiddenLayerSimilarity: 512
      numChannelsOutputLayerSimilarity: 1024

      playTypes:
        PLAYOUT:
          forTraining: false
          numSimulations: 50
          rootExplorationFraction: 0.0  # 0.0 means switched off
          temperatureRoot: 0.0
          gumbelActionSelection: false
        HYBRID:
          forTraining: true
          numEpisodes: 2
          numParallelGamesPlayed: 250
          tdSteps:  10000
          forTdStep0ValueTraining: false
          forTdStep0PolicyTraining: true
          numSimulations: 50
          numSimulationsHybrid: 50

          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
          # dirichlet noise is added before mcts
          rootDirichletAlpha: 0.1
          rootExplorationFraction: 0.25  # 0.0 means switched off

          # temperatureRoot applies a temperature to the policy target after mcts and after storing it in game statistics but
          # before drawing the action from the policy target (a little different from the paper)
          # it helps to explore the game tree where the policy rules out bad moves and makes the agent blind to what could happen after such a move
          temperatureRoot: 2.0
          gumbelActionSelection: false

        BEST_EFFORT:
          forTraining: true
          numEpisodes: 2
          numParallelGamesPlayed: 250
          tdSteps:  10000
          numSimulations: 50
          rootExplorationFraction: 0.0  # 0.0 means switched off
          gumbelActionSelection: true


