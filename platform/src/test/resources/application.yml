
#ai:
#  djl:
#    logging:
#      level: debug

muzero:
  activeGame: TEST
  run: none
  games:
    TEST:
      modelName: MuZero-Test
      gameClassName: ai.enpasos.muzero.platform.agent.e_experience.TestGame

      actionClassName: ai.enpasos.muzero.platform.agent.e_experience.TestAction
      playerMode: TWO_PLAYERS
      networkWithRewardHead: false
      valueInterval: [ -1,1 ]
      numObservationLayers: 3
      numActionLayers: 1


      numberOfTrainingStepsPerEpoch: 40

      numUnrollSteps: 5

      discount: 1.0
      weightDecay: 0.0001
      lrInit: 0.0001

      # numberTrainingStepsOnStart: 0

      knownBoundsType: FROM_VALUES

      inferenceDeviceType: GPU
      outputDir: ./memory/tictactoe/
      size: 3
      maxMoves: 9  # size*size
      boardHeight: 3 # size
      boardWidth: 3 # size
      actionSpaceSize: 9  # size*size

      # without symmetry usage
      # symmetryType: NONE
      # batchSize: 2048

      # using the square symmetry of the board
      symmetryType: SQUARE
      batchSize: 256

      gameBufferWritingFormat: ZIPPED_PROTOCOL_BUFFERS # alternative: ZIPPED_JSON

      # Gumbel MuZero parameters
      initialGumbelM: 4
      cVisit: 20  # 50 in paper
      cScale: 1.0  # 1.0 in paper

      numResiduals: 6
      broadcastEveryN: 8  # out of range here

      numChannels: 256
      numBottleneckChannels: 128

      valueLossWeight: 1

      numberOfTrainingSteps: 50000

      windowSize: 10000

      numChannelsHiddenLayerSimilarity: 250
      numChannelsOutputLayerSimilarity: 500

      replayFraction: 0.0

      offPolicyCorrectionOn: true
      offPolicyRatioLimit: 10.0

      playTypes:
        PLAYOUT:
          forTraining: false
          numSimulations: 20
          rootExplorationFraction: 0.0  # 0.0 means switched off
          temperatureRoot: 0.0
          gumbelActionSelection: true
        HYBRID:
          forTraining: true
          numParallelGamesPlayed: 1000
          tdSteps: 9 # 9 # size * size
          forTdStep0ValueTraining: false
          forTdStep0PolicyTraining: true
          numSimulations: 20
          numSimulationsHybrid: 20

          # dirichlet noise on root node helps to explore the game tree where the policy is temporarily underestimating some moves
          # dirichlet noise is added before mcts
          rootDirichletAlpha: 1.2
          rootExplorationFraction: 0.25  # 0.0 means switched off

          # temperatureRoot applies a temperature to the policy target after mcts and after storing it in game statistics but
          # before drawing the action from the policy target (different from the paper)
          # it helps to explore the game tree where the policy rules out bad moves and makes the agent blind to what could happen after such a move
          temperatureRoot: 2.0   # was 5
          gumbelActionSelection: true
          gumbelActionSelectionOnExploring: false



